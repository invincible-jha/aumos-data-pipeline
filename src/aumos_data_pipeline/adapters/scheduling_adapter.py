"""Orchestration scheduling adapter.

Generates DAG definitions for Airflow and workflow specs for Temporal from
pipeline configurations. Handles cron expression parsing and validation,
dependency chain resolution, retry policy configuration, and pipeline run
status tracking.

Supported orchestrators:
- Apache Airflow (DAG Python file generation)
- Temporal (workflow definition dict for SDK registration)
- Generic cron (for standalone schedulers)
"""

import datetime
import re
import uuid
from typing import Any

from aumos_common.observability import get_logger

from aumos_data_pipeline.core.interfaces import SchedulingAdapterProtocol

logger = get_logger(__name__)

# Valid cron field ranges: minute, hour, day-of-month, month, day-of-week
_CRON_FIELD_RANGES = [
    (0, 59),   # minute
    (0, 23),   # hour
    (1, 31),   # day of month
    (1, 12),   # month
    (0, 7),    # day of week (0 and 7 are Sunday)
]

# Default retry policy
_DEFAULT_MAX_RETRIES = 3
_DEFAULT_RETRY_DELAY_SECONDS = 300
_DEFAULT_TIMEOUT_SECONDS = 3600


class SchedulingAdapter:
    """Generates and manages pipeline schedules for Airflow and Temporal.

    Implements SchedulingAdapterProtocol. Converts a pipeline_config dict into
    a fully-formed Airflow DAG Python string or a Temporal workflow definition.
    Also tracks pipeline run status in an in-memory registry (backed by a
    database in production).

    Pipeline config schema::

        {
            "pipeline_id": "my_pipeline",
            "display_name": "My Pipeline",
            "schedule": "0 3 * * *",          # Cron expression
            "orchestrator": "airflow",          # 'airflow' | 'temporal' | 'cron'
            "stages": ["ingest", "clean", "transform"],
            "dependencies": {"clean": ["ingest"], "transform": ["clean"]},
            "retry_policy": {
                "max_retries": 3,
                "retry_delay_seconds": 300,
                "timeout_seconds": 3600,
            },
            "sla_seconds": 7200,               # Expected completion time
            "tags": ["crm", "daily"],
        }
    """

    def __init__(self) -> None:
        """Initialize the scheduling adapter with an empty run registry."""
        # In-memory run status registry: {run_id: run_status_dict}
        self._run_registry: dict[str, dict[str, Any]] = {}

    async def generate_airflow_dag(
        self,
        pipeline_config: dict[str, Any],
        tenant_id: uuid.UUID,
    ) -> str:
        """Generate a Python Airflow DAG file string from a pipeline config.

        The returned string is a complete, importable Python module that
        Airflow's DAG discovery mechanism can pick up directly.

        Args:
            pipeline_config: Pipeline definition (see class docstring).
            tenant_id: Tenant context (embedded as DAG owner).

        Returns:
            Python source code string defining the Airflow DAG.
        """
        pipeline_id: str = pipeline_config.get("pipeline_id", "aumos_pipeline")
        display_name: str = pipeline_config.get("display_name", pipeline_id)
        schedule: str = pipeline_config.get("schedule", "@daily")
        stages: list[str] = pipeline_config.get("stages", [])
        dependencies: dict[str, list[str]] = pipeline_config.get("dependencies", {})
        retry_policy: dict[str, Any] = pipeline_config.get("retry_policy", {})
        sla_seconds: int = pipeline_config.get("sla_seconds", _DEFAULT_TIMEOUT_SECONDS)
        tags: list[str] = pipeline_config.get("tags", [])

        max_retries: int = retry_policy.get("max_retries", _DEFAULT_MAX_RETRIES)
        retry_delay: int = retry_policy.get("retry_delay_seconds", _DEFAULT_RETRY_DELAY_SECONDS)
        timeout: int = retry_policy.get("timeout_seconds", _DEFAULT_TIMEOUT_SECONDS)

        # Validate and resolve stage dependency order
        ordered_stages = self._resolve_dependency_order(stages, dependencies)

        # Build task definitions
        task_defs = self._build_airflow_tasks(ordered_stages, dependencies, pipeline_id, tenant_id)

        dag_code = f'''"""Auto-generated Airflow DAG for AumOS pipeline: {pipeline_id}
Generated by aumos-data-pipeline scheduling adapter.
Tenant: {tenant_id}
"""

from __future__ import annotations

import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

DEFAULT_ARGS = {{
    "owner": "{tenant_id}",
    "depends_on_past": False,
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": {max_retries},
    "retry_delay": datetime.timedelta(seconds={retry_delay}),
    "execution_timeout": datetime.timedelta(seconds={timeout}),
    "sla": datetime.timedelta(seconds={sla_seconds}),
}}

with DAG(
    dag_id="{pipeline_id}",
    description="{display_name}",
    schedule_interval="{schedule}",
    start_date=days_ago(1),
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags={json_list(tags + ["aumos", str(tenant_id)])},
    max_active_runs=1,
) as dag:

{task_defs}
'''

        logger.info(
            "Airflow DAG generated",
            pipeline_id=pipeline_id,
            stage_count=len(stages),
            schedule=schedule,
            tenant_id=str(tenant_id),
        )
        return dag_code

    async def generate_temporal_workflow(
        self,
        pipeline_config: dict[str, Any],
        tenant_id: uuid.UUID,
    ) -> dict[str, Any]:
        """Generate a Temporal workflow definition dict from a pipeline config.

        The returned dict is passed to the Temporal Python SDK's
        ``worker.register_workflow_implementation_type`` or used to construct
        a ``WorkflowHandle``.

        Args:
            pipeline_config: Pipeline definition (see class docstring).
            tenant_id: Tenant context.

        Returns:
            Dict with Temporal workflow spec:
                - workflow_id: Unique workflow identifier
                - task_queue: Temporal task queue name
                - workflow_type: Workflow class name
                - activities: Ordered list of activity definitions
                - retry_policy: RetryPolicy parameters
                - schedule: Cron schedule string
        """
        pipeline_id: str = pipeline_config.get("pipeline_id", "aumos_pipeline")
        stages: list[str] = pipeline_config.get("stages", [])
        dependencies: dict[str, list[str]] = pipeline_config.get("dependencies", {})
        retry_policy: dict[str, Any] = pipeline_config.get("retry_policy", {})
        schedule: str = pipeline_config.get("schedule", "")

        ordered_stages = self._resolve_dependency_order(stages, dependencies)

        activities = [
            {
                "name": f"aumos_{stage}_activity",
                "start_to_close_timeout_seconds": retry_policy.get(
                    "timeout_seconds", _DEFAULT_TIMEOUT_SECONDS
                ),
                "retry_policy": {
                    "maximum_attempts": retry_policy.get("max_retries", _DEFAULT_MAX_RETRIES),
                    "initial_interval_seconds": retry_policy.get(
                        "retry_delay_seconds", _DEFAULT_RETRY_DELAY_SECONDS
                    ),
                    "backoff_coefficient": 2.0,
                    "maximum_interval_seconds": retry_policy.get(
                        "retry_delay_seconds", _DEFAULT_RETRY_DELAY_SECONDS
                    ) * 10,
                },
                "stage": stage,
                "pipeline_id": pipeline_id,
                "tenant_id": str(tenant_id),
            }
            for stage in ordered_stages
        ]

        workflow_spec: dict[str, Any] = {
            "workflow_id": f"{pipeline_id}__{tenant_id}",
            "task_queue": f"aumos-pipeline-{pipeline_id}",
            "workflow_type": "AumosPipelineWorkflow",
            "activities": activities,
            "retry_policy": {
                "maximum_attempts": retry_policy.get("max_retries", _DEFAULT_MAX_RETRIES),
                "initial_interval_seconds": retry_policy.get(
                    "retry_delay_seconds", _DEFAULT_RETRY_DELAY_SECONDS
                ),
                "backoff_coefficient": 2.0,
            },
            "schedule": schedule or None,
            "pipeline_config": pipeline_config,
            "tenant_id": str(tenant_id),
        }

        logger.info(
            "Temporal workflow generated",
            pipeline_id=pipeline_id,
            activity_count=len(activities),
            tenant_id=str(tenant_id),
        )
        return workflow_spec

    async def validate_cron(self, cron_expression: str) -> dict[str, Any]:
        """Parse and validate a cron expression.

        Supports standard 5-field cron syntax (minute hour dom month dow)
        as well as special strings (@daily, @hourly, @weekly, @monthly).

        Args:
            cron_expression: Cron schedule string to validate.

        Returns:
            Dict with:
                - valid: bool
                - expression: Normalized expression string
                - next_runs: List of ISO-formatted next 5 run times (approximate)
                - description: Human-readable description
                - error: Error message if invalid (empty string if valid)
        """
        special_aliases: dict[str, str] = {
            "@yearly": "0 0 1 1 *",
            "@annually": "0 0 1 1 *",
            "@monthly": "0 0 1 * *",
            "@weekly": "0 0 * * 0",
            "@daily": "0 0 * * *",
            "@midnight": "0 0 * * *",
            "@hourly": "0 * * * *",
        }

        normalized = special_aliases.get(cron_expression.lower(), cron_expression)
        fields = normalized.split()

        if len(fields) != 5:
            return {
                "valid": False,
                "expression": cron_expression,
                "next_runs": [],
                "description": "",
                "error": f"Cron expression must have exactly 5 fields, got {len(fields)}",
            }

        validation_errors: list[str] = []
        for index, (field, (min_val, max_val)) in enumerate(zip(fields, _CRON_FIELD_RANGES)):
            error = self._validate_cron_field(field, min_val, max_val, index)
            if error:
                validation_errors.append(error)

        if validation_errors:
            return {
                "valid": False,
                "expression": cron_expression,
                "next_runs": [],
                "description": "",
                "error": "; ".join(validation_errors),
            }

        description = self._describe_cron(fields)
        next_runs = self._compute_next_runs(fields, count=5)

        return {
            "valid": True,
            "expression": normalized,
            "next_runs": next_runs,
            "description": description,
            "error": "",
        }

    async def track_run(
        self,
        pipeline_id: str,
        run_id: str,
        status: str,
        metadata: dict[str, Any],
        tenant_id: uuid.UUID,
    ) -> dict[str, Any]:
        """Record or update a pipeline run's status in the registry.

        Args:
            pipeline_id: Logical pipeline identifier.
            run_id: Unique run identifier (e.g., Airflow dag_run_id).
            status: Run status ('pending', 'running', 'completed', 'failed').
            metadata: Additional context (start_time, end_time, error, etc.).
            tenant_id: Tenant context.

        Returns:
            The updated run record dict.
        """
        registry_key = f"{tenant_id}__{pipeline_id}__{run_id}"
        now = datetime.datetime.now(tz=datetime.timezone.utc).isoformat()

        existing = self._run_registry.get(registry_key, {})
        run_record: dict[str, Any] = {
            **existing,
            "pipeline_id": pipeline_id,
            "run_id": run_id,
            "tenant_id": str(tenant_id),
            "status": status,
            "updated_at": now,
            **metadata,
        }

        if "created_at" not in run_record:
            run_record["created_at"] = now

        self._run_registry[registry_key] = run_record

        logger.info(
            "Pipeline run status tracked",
            pipeline_id=pipeline_id,
            run_id=run_id,
            status=status,
            tenant_id=str(tenant_id),
        )
        return run_record

    async def get_run_history(
        self,
        pipeline_id: str,
        tenant_id: uuid.UUID,
        limit: int = 20,
    ) -> list[dict[str, Any]]:
        """Retrieve recent run history for a pipeline.

        Args:
            pipeline_id: Logical pipeline identifier.
            tenant_id: Tenant context.
            limit: Maximum number of runs to return (most recent first).

        Returns:
            List of run record dicts, ordered by updated_at descending.
        """
        prefix = f"{tenant_id}__{pipeline_id}__"
        runs = [
            record
            for key, record in self._run_registry.items()
            if key.startswith(prefix)
        ]
        runs.sort(key=lambda r: r.get("updated_at", ""), reverse=True)
        return runs[:limit]

    def _resolve_dependency_order(
        self,
        stages: list[str],
        dependencies: dict[str, list[str]],
    ) -> list[str]:
        """Topologically sort pipeline stages by dependency.

        Uses Kahn's algorithm to produce a valid execution order.

        Args:
            stages: All stage names.
            dependencies: {stage: [upstream_stages]} edges.

        Returns:
            Stages in topological order.

        Raises:
            ValueError: If a dependency cycle is detected.
        """
        in_degree: dict[str, int] = {stage: 0 for stage in stages}
        adjacency: dict[str, list[str]] = {stage: [] for stage in stages}

        for stage, upstream_list in dependencies.items():
            for upstream in upstream_list:
                if upstream in adjacency:
                    adjacency[upstream].append(stage)
                    in_degree[stage] = in_degree.get(stage, 0) + 1

        queue = [stage for stage in stages if in_degree[stage] == 0]
        ordered: list[str] = []

        while queue:
            node = queue.pop(0)
            ordered.append(node)
            for downstream in adjacency.get(node, []):
                in_degree[downstream] -= 1
                if in_degree[downstream] == 0:
                    queue.append(downstream)

        if len(ordered) != len(stages):
            raise ValueError(
                "Dependency cycle detected in pipeline stages. "
                f"Unresolved stages: {set(stages) - set(ordered)}"
            )

        return ordered

    def _build_airflow_tasks(
        self,
        ordered_stages: list[str],
        dependencies: dict[str, list[str]],
        pipeline_id: str,
        tenant_id: uuid.UUID,
    ) -> str:
        """Build the Python task definition block for an Airflow DAG.

        Args:
            ordered_stages: Topologically sorted list of stage names.
            dependencies: {stage: [upstream_stages]} edges.
            pipeline_id: Pipeline identifier for task naming.
            tenant_id: Tenant UUID for API calls.

        Returns:
            Python source code block defining all PythonOperator tasks.
        """
        lines: list[str] = []

        for stage in ordered_stages:
            task_id = f"{pipeline_id}_{stage}"
            function_name = f"run_{stage}"
            upstream_stages = dependencies.get(stage, [])

            lines.append(f"    def {function_name}(**context):")
            lines.append(f'        """Execute the {stage} stage of {pipeline_id}."""')
            lines.append(f"        import httpx")
            lines.append(f"        response = httpx.post(")
            lines.append(f'            "http://aumos-data-pipeline/api/v1/pipeline/stage",')
            lines.append(f"            json={{")
            lines.append(f'                \"stage\": \"{stage}\",')
            lines.append(f'                \"pipeline_id\": \"{pipeline_id}\",')
            lines.append(f'                \"tenant_id\": \"{tenant_id}\",')
            lines.append(f'                \"run_id\": context[\"run_id\"],')
            lines.append(f"            }},")
            lines.append(f'            headers={{"X-Tenant-ID": "{tenant_id}"}},')
            lines.append(f"            timeout=3600,")
            lines.append(f"        )")
            lines.append(f"        response.raise_for_status()")
            lines.append(f"")
            lines.append(f"    {task_id}_task = PythonOperator(")
            lines.append(f'        task_id="{task_id}",')
            lines.append(f"        python_callable={function_name},")
            lines.append(f"    )")
            lines.append(f"")

        # Wire task dependencies
        for stage in ordered_stages:
            upstream_stages = dependencies.get(stage, [])
            for upstream in upstream_stages:
                upstream_task_id = f"{pipeline_id}_{upstream}"
                current_task_id = f"{pipeline_id}_{stage}"
                lines.append(f"    {upstream_task_id}_task >> {current_task_id}_task")

        return "\n".join(lines)

    def _validate_cron_field(
        self,
        field: str,
        min_val: int,
        max_val: int,
        field_index: int,
    ) -> str:
        """Validate a single cron field against its allowed range.

        Args:
            field: The cron field string (may contain *, /, -, ,).
            min_val: Minimum allowed integer value.
            max_val: Maximum allowed integer value.
            field_index: Position index (0=minute, 1=hour, etc.) for error messages.

        Returns:
            Empty string if valid, or an error message string.
        """
        field_names = ["minute", "hour", "day-of-month", "month", "day-of-week"]
        field_name = field_names[field_index] if field_index < len(field_names) else f"field[{field_index}]"

        if field == "*":
            return ""

        # Step values: */n or m-n/s
        if "/" in field:
            parts = field.split("/")
            if len(parts) != 2:
                return f"Invalid step syntax in {field_name}: '{field}'"
            base, step_str = parts
            if not step_str.isdigit():
                return f"Step value must be an integer in {field_name}: '{step_str}'"
            step = int(step_str)
            if step <= 0:
                return f"Step value must be positive in {field_name}: {step}"
            if base != "*":
                error = self._validate_cron_field(base, min_val, max_val, field_index)
                if error:
                    return error
            return ""

        # Range values: m-n
        if "-" in field:
            parts = field.split("-")
            if len(parts) != 2:
                return f"Invalid range syntax in {field_name}: '{field}'"
            low_str, high_str = parts
            if not (low_str.isdigit() and high_str.isdigit()):
                return f"Range bounds must be integers in {field_name}: '{field}'"
            low, high = int(low_str), int(high_str)
            if not (min_val <= low <= max_val and min_val <= high <= max_val):
                return f"Range [{low}-{high}] out of bounds [{min_val}-{max_val}] in {field_name}"
            if low > high:
                return f"Range start must be <= end in {field_name}: {low} > {high}"
            return ""

        # List values: a,b,c
        if "," in field:
            for item in field.split(","):
                error = self._validate_cron_field(item, min_val, max_val, field_index)
                if error:
                    return error
            return ""

        # Literal integer
        if not field.isdigit():
            return f"Expected integer or special character in {field_name}: '{field}'"
        value = int(field)
        if not (min_val <= value <= max_val):
            return f"Value {value} out of range [{min_val}-{max_val}] in {field_name}"
        return ""

    def _describe_cron(self, fields: list[str]) -> str:
        """Generate a human-readable description for a cron expression.

        Args:
            fields: List of 5 cron field strings.

        Returns:
            Natural language description string.
        """
        minute, hour, dom, month, dow = fields

        if minute == "0" and hour == "0" and dom == "*" and month == "*" and dow == "*":
            return "Every day at midnight"
        if minute == "0" and dom == "*" and month == "*" and dow == "*":
            return f"Every day at {hour}:00"
        if minute == "0" and hour == "0" and dom == "1" and month == "*" and dow == "*":
            return "First day of every month at midnight"
        if minute == "0" and hour == "0" and dom == "*" and month == "*" and dow == "0":
            return "Every Sunday at midnight"
        if minute == "*":
            return "Every minute"
        if hour == "*":
            return f"Every hour at minute {minute}"

        return f"At minute {minute} of hour {hour} (dom={dom} month={month} dow={dow})"

    def _compute_next_runs(self, fields: list[str], count: int = 5) -> list[str]:
        """Approximate the next N run times for a cron expression.

        Uses a brute-force minute-by-minute search starting from now.
        This is accurate for standard expressions but does not handle all
        edge cases (e.g., last-day-of-month specifiers).

        Args:
            fields: List of 5 cron field strings.
            count: Number of upcoming run times to compute.

        Returns:
            List of ISO-formatted datetime strings.
        """
        minute_field, hour_field, _, _, _ = fields
        results: list[str] = []

        now = datetime.datetime.now(tz=datetime.timezone.utc).replace(second=0, microsecond=0)
        current = now + datetime.timedelta(minutes=1)
        max_iterations = 60 * 24 * 370  # Search up to one year ahead

        for _ in range(max_iterations):
            if len(results) >= count:
                break
            if self._cron_field_matches(minute_field, current.minute, 0, 59):
                if self._cron_field_matches(hour_field, current.hour, 0, 23):
                    results.append(current.isoformat())
            current += datetime.timedelta(minutes=1)

        return results

    def _cron_field_matches(self, field: str, value: int, min_val: int, max_val: int) -> bool:
        """Check whether a cron field matches a specific integer value.

        Args:
            field: Cron field string.
            value: Integer to test.
            min_val: Minimum valid value for this field.
            max_val: Maximum valid value for this field.

        Returns:
            True if the field matches value.
        """
        if field == "*":
            return True
        if "," in field:
            return any(
                self._cron_field_matches(f, value, min_val, max_val)
                for f in field.split(",")
            )
        if "/" in field:
            base, step_str = field.split("/", 1)
            step = int(step_str)
            start = min_val if base == "*" else int(base.split("-")[0])
            return value >= start and (value - start) % step == 0
        if "-" in field:
            low, high = map(int, field.split("-", 1))
            return low <= value <= high
        if field.isdigit():
            return int(field) == value
        return False


def json_list(items: list[str]) -> str:
    """Format a Python list of strings as a Python list literal.

    Args:
        items: String items to format.

    Returns:
        Python list literal string, e.g. '["a", "b"]'.
    """
    formatted = ", ".join(f'"{item}"' for item in items)
    return f"[{formatted}]"
